Step: 1 inatall awscli

apt update && apt install awscli -y


Step: 2 setup aws keys

aws configure 

Step: 3 install kubectl and eksctl 

sudo curl --location -o /usr/local/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl

sudo chmod +x /usr/local/bin/kubectl

kubectl version --short --client

curl --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp

sudo mv -v /tmp/eksctl /usr/local/bin

eksctl version

export AWS_REGION=$(curl --silent http://169.254.169.254/latest/meta-data/placement/region) && echo $AWS_REGION

Step: 4 Create EKS Cluster 

Method: 1  imperative, using eksctl command 

eksctl create cluster \
--name dev-cluster \
--nodegroup-name dev-nodes \
--node-type t3.medium \
--nodes 3 \
--nodes-min 1 \
--nodes-max 4 \
--managed \
--version 1.23 \
--region ${AWS_REGION}


kubectl get nodes



Method: 2 Declarative, By writing YAML file

ssh-keygen

vi eks.yaml

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: testcluster
  region: ap-south-1
  version: "1.23"
nodeGroups:
  - name: ng-1
    instanceType: t2.medium
    maxSize: 2
    minSize: 1
    volumeSize: 30
    volumeType: gp3
    ssh:
      allow: true
  - name: ng-2
    instanceType: t2.medium
    desiredCapacity: 2
    volumeSize: 30
    volumeType: gp3
    ssh:
      allow: true

eksctl create cluster -f eks.yaml

eksctl delete cluster -f eks.yaml


Download kube/config file now 
	

###   Install EBS CSI drivers


create policy with given policy.josn file
goto IAM create policy using json file 

policy name should look like this
arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy


kubectl create secret generic aws-secret \
    --namespace kube-system \
    --from-literal "key_id=AKIAXLLKKPGIZ34MSDUC" \
    --from-literal "access_key=FlQZK0Gn64mt8P0VBw/oIrCzrKTNQaJoPl+qTcz7"

kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.19"

kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver



check pods are running 
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver


Task: 1 

vi pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi

kubectl create -f pvc.yaml


Task: 2 create pod now

vi pvc-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
     app: myapp
     env: dev
spec:
 containers:
 - name: con1
   image: nginx:latest
   ports:
   - containerPort: 80
   volumeMounts:
   - name: myvol
     mountPath: /mnt
 volumes:
 - name: myvol
   persistentVolumeClaim:
      claimName: myclaim

kubectl create -f pvc-pod.yaml

kubectl get pvc
kubectl get pod


clean up
kubectl delete -f pvc.yaml -f pvc-pod.yaml
kubectl delete -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.19"







